{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook you will learn about the **AR-CNN** - a novel self-correcting, autoregressive model that uses a convolutional neural network in its architecture. By the end of this notebook, you will have trained and ran inference on your very own custom model. This notebook dives into details on the model and assumes a moderate level of understanding of machine learning concepts; as a result, we encourage you to read the introductory [learning capsules](https://console.aws.amazon.com/deepcomposer/home?region=us-east-1#learningCapsules) before going through this notebook.\n",
    "\n",
    "Traditionally, there have been two primary approaches to generating music with deep neural network-based generative models. One treats music generation as an image generation problem,\n",
    "while the other treats music as a time series generation problem analogous to autoregressive language modeling. The AR-CNN model uses elements from both approaches to generate music. We view each piece of music as a piano roll (an image representation of music), but generate each note (i.e. pixel) autoregressively.\n",
    "\n",
    "Generating images autoregressively has been an area of interest to researchers. \n",
    "* Orderless NADE showcased an approach to generating images assuming ordering-invariance in the next pixel to be added.  \n",
    "* PixelCNN demonstrated with a fixed row by row ordering that an autoregressive approach can generate convincing results for CIFAR-10.\n",
    "\n",
    "In the music domain, CocoNET - the algorithm behind Google’s Bach Doodle - adopts an approach similar to orderless NADE, but using Gibbs Sampling to obtain inference results. \n",
    "\n",
    "One common theme with autoregressive approaches, however, is that they are very prone to accumulation of error. Our approach is novel in that the model is trained to detect mistakes - including those it made itself - and fix them. We do this by viewing music generation as a series of **edit events** which can be either the addition of a new note or removal of an existing note. An **edit sequence** is a series of **edit events** and every edit sequence can directly correspond to a piece of music. By training our model to view the problem as edit events rather than as an entire image or just the addition of notes, we found that our model is able to offset accumulation of error and generate higher quality music.\n",
    "\n",
    "Now that you understand the basic theory behind our approach, let’s dive into the practical code. In the next section we discuss and show examples using the piano roll format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependencies\n",
    "First, let's install and import all of the python packages we will use throughout the tutorial.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The MIT-Zero License\n",
    "\n",
    "# Copyright 2020 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n",
    "\n",
    "# Permission is hereby granted, free of charge, to any person obtaining a copy\n",
    "# of this software and associated documentation files (the \"Software\"), to deal\n",
    "# in the Software without restriction, including without limitation the rights\n",
    "# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
    "# copies of the Software, and to permit persons to whom the Software is\n",
    "# furnished to do so.\n",
    "\n",
    "# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
    "# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
    "# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
    "# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
    "# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
    "# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\n",
    "# THE SOFTWARE.\n",
    "\n",
    "\n",
    "# Create the environment and install required packages\n",
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "import glob\n",
    "import json\n",
    "import numpy as np\n",
    "import keras\n",
    "from enum import Enum\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D, concatenate, BatchNormalization, Dropout\n",
    "from keras.optimizers import Adam, RMSprop\n",
    "from keras import backend as K\n",
    "from random import randrange\n",
    "import random\n",
    "import math\n",
    "import pypianoroll\n",
    "from utils.midi_utils import play_midi, plot_pianoroll, get_music_metrics, process_pianoroll, process_midi\n",
    "from constants import Constants\n",
    "from augmentation import AddAndRemoveAPercentageOfNotes\n",
    "from data_generator import PianoRollGenerator\n",
    "from utils.generate_training_plots import GenerateTrainingPlots\n",
    "from inference import Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Summary\n",
    "In this tutorial, we use the [`JSB-Chorales-dataset`](http://www-etud.iro.umontreal.ca/~boulanni/icml2012), comprising 229 chorale snippets. A chorale is a hymn that is usually sung with a single voice playing a simple melody and three lower voices providing harmony. In this dataset, these voices are represented by four piano tracks.\n",
    "\n",
    "In case, you want to train the ArCnn model on your own dataset, please replace the current **data_dir** path with your directory to midi files.\n",
    "Let's listen to a song from this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get The List Of Midi Files\n",
    "data_dir = 'data/*.mid'\n",
    "midi_files = glob.glob(data_dir)\n",
    "random_midi = randrange(len(midi_files))\n",
    "play_midi(midi_files[random_midi])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Format - Piano Roll"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the purpose of this tutorial, we represent music from the JSB-Chorales dataset in the piano roll format.\n",
    "\n",
    "\n",
    "A **piano roll** is a discrete, image-like representation of music which can be viewed as a two-dimensional grid with **\"Time\"** on the horizontal axis and **\"Pitch\"** on the vertical axis. In our use case, the presence of a pixel in any particular cell in this grid indicates if a note was played or not at that time and pitch.\n",
    "Let us look at a few piano rolls in our dataset. In this example, a single piano roll track has 128 discrete time steps and 128 pitches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/pianoroll.png\" alt=\"Dataset summary\" width=\"800\">\n",
    "\n",
    "ArCnn model when comes across midi files with multiple tracks, all the tracks are merged to form a single track and this can be visualized below.\n",
    "\n",
    "<img src=\"images/merged_pianoroll.png\" alt=\"Merged Piano roll\" width=\"300\">\n",
    "\n",
    "You might notice this representation looks similar to an image. While the sequence of notes is often the natural way that people view music, many modern machine learning models instead treat music as images and leverage existing techniques within the computer vision domain. You will see such techniques used in our architecture later in this tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Why 128 time steps?**\n",
    "\n",
    "For the purpose of this tutorial, we sample eight non-empty bars (https://en.wikipedia.org/wiki/Bar_(music)) from each song in the JSB-Chorales dataset. A **bar** (or **measure**) is a unit of composition and contains four beats for songs in our particular dataset (our songs are all in 4/4 time) :\n",
    "\n",
    "We’ve found that using a resolution of four time steps per beat captures enough of the musical detail in this dataset.\n",
    "\n",
    "This yields...\n",
    "\n",
    "$$ \\frac{4\\;timesteps}{1\\;beat} * \\frac{4\\;beats}{1\\;bar} * \\frac{8\\;bars}{1} = 128\\;timesteps $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create The Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Midi File Samples\n",
    "def generate_samples(midi_files, bars, beats_per_bar, beat_resolution, bars_shifted_per_sample):\n",
    "    \"\"\"\n",
    "    dataset_files: All files in the dataset\n",
    "    return: piano roll samples sized to X bars\n",
    "    \"\"\"\n",
    "    timesteps_per_nbars = bars * beats_per_bar * beat_resolution\n",
    "    time_steps_shifted_per_sample = bars_shifted_per_sample * beats_per_bar * beat_resolution\n",
    "    samples = []\n",
    "    for midi_file in midi_files:\n",
    "        pianoroll = process_midi(midi_file, beat_resolution) # Parse the midi file and get the piano roll\n",
    "        samples.extend(process_pianoroll(pianoroll, time_steps_shifted_per_sample, timesteps_per_nbars))\n",
    "    return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Input Midi Files To Tensors\n",
    "dataset_samples = generate_samples(midi_files, Constants.bars, Constants.beats_per_bar, \n",
    "                                   Constants.beat_resolution, \n",
    "                                   Constants.bars_shifted_per_sample)\n",
    "# Shuffle The Dataset\n",
    "random.shuffle(dataset_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize A Random Piano roll\n",
    "random_pianoroll = dataset_samples[randrange(len(dataset_samples))]\n",
    "plot_pianoroll(pianoroll = random_pianoroll,\n",
    "               beat_resolution = 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Augmentation\n",
    "\n",
    "The augmented **input piano roll** is created by adding and removing notes from the original piano roll. By keeping the original piano roll as the target, the model learns what edit events (i.e. notes to add and remove) are needed to recreate from the augmented piano roll. The augmented piano roll can represent a user input melody which has some mistakes / off-tune notes that need to be corrected. In this way, the model learns how to fix/improve the input.\n",
    "During training, the data generator creates (input, target) pairs by applying augmentations on the piano rolls present in the dataset. In each epoch, different notes are added and removed from original piano rolls to form the augmented piano rolls (as these notes are added/removed in random pixels). This means that we will have a new set of augmented piano rolls for each epoch, and this effectively creates an unlimited input training dataset. \n",
    "\n",
    "There can be multiple augmented piano rolls for a single original piano roll, and this can be configured using the parameter - **“samples_per_ground_truth_data_item”** in **constants.py**. Details of adding and removing notes during augmentation are explained below.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing Notes From The Original Piano Roll\n",
    "\n",
    "Notes are randomly removed from the original piano roll to form the augmented piano roll. The model learns that it needs to add these notes in the augmented piano roll to recreate the original piano roll. This teaches the model how to fill in missing notes. The percentage of original notes to remove is determined by sampling from a uniform distribution between a lower and upper bound. The default lower bound of notes to remove is 0% as this helps the model learn that it doesn’t need to add notes to the input when the input is already “perfect”. The default upper bound is 100% as this helps the model create music when nothing is given as input (the unconditioned music generation case). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![SegmentLocal](images/removenotes.gif \"segment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding Notes To The Original Piano Roll \n",
    "\n",
    "Notes are randomly added to the original piano roll to form the augmented piano roll. The model learns that it needs to remove these notes in the augmented piano roll to recreate the original. This teaches the model how to remove unnecessary or off-tune notes. The percentage of extra notes to add is determined by sampling from a uniform distribution between a lower and upper bound (similar to the removing notes case). The default lower bound of notes to add is 0% of the current empty notes. This teaches the model to remove no notes when the input is already “perfect”. The default upper bound of notes to add is 1.5% of the current empty pixels (that do not have a note). This upper percentage may seem small, but since the percentage is out of the total empty pixels (which are usually far greater than the number of notes), the upper bound ends up being sufficiently large. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![SegmentLocal](images/addnotes.gif \"segment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For both percentage of notes to add and remove, sampling is done from a uniform distribution to ensure that the model sees different potential states equally often. During training, this equal representation helps the model learn how to fill in or remove different numbers of notes, and how to recreate the original from any stage of the input. This is useful during the iterative inference process which we describe in more detail in the Inference section.\n",
    "\n",
    "Both adding and removing notes is performed together on each piano roll. The sampling lower bound and sampling upper bound parameters for these can be changed via the parameters -  **“sampling_lower_bound_remove”**, **“sampling_upper_bound_remove”**, **“sampling_lower_bound_add”**, and **“sampling_upper_bound_add”**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_lower_bound_remove = 0\n",
    "sampling_upper_bound_remove = 100\n",
    "sampling_lower_bound_add = 1\n",
    "sampling_upper_bound_add = 1.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Function\n",
    "\n",
    "Rather than using a traditional loss function such as binary crossentropy, we calculate a custom loss function for our model. In our augmentation we both add extraneous notes and remove existing notes from the piano roll. Our end goal is to have the model pick the next **edit event**(i.e. the next note to add or remove) so that we can take the input piano roll and bring it closer to the original piano roll, also known as the **target piano roll**. Notice that the model could pick any one of the extraneous or missing notes to bring the input piano roll closer to the target piano roll. These extraneous or missing notes is the **symmetric difference** between the input and target piano rolls. We can calculate the symmetric difference as the **exclusive-or** between the input and target piano rolls. Assuming that choosing any of the notes in the symmetric difference is equally likely, the model’s goal is to minimize the difference between its output and a uniform distribution for the probabilities of each of those notes. This difference in distributions can be calculated as the **Kullback–Leibler divergence**. Thus our loss function is the difference between the model’s output and the uniform distribution of all pixels/note probabilities in the symmetric difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Customized Loss function\n",
    "class Loss():\n",
    "    @staticmethod\n",
    "    def built_in_softmax_kl_loss(target, output):\n",
    "        '''\n",
    "        Custom Loss Function\n",
    "        :param target: ground truth values\n",
    "        :param output: predicted values\n",
    "        :return kullback_leibler_divergence loss\n",
    "        '''\n",
    "        target = K.flatten(target)\n",
    "        output = K.flatten(output)\n",
    "        target = target / K.sum(target)\n",
    "        output = K.softmax(output)\n",
    "        return keras.losses.kullback_leibler_divergence(target, output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our Model architecture is adapted from the U-Net architecture (a popular CNN that is used extensively in the computer vision domain), consisting of an **“encoder”** that maps the single track music data (represented as piano roll images) to a relatively lower dimensional “latent space“ and a **”decoder“** that maps the latent space back to multi-track music data.\n",
    "\n",
    "Here are the inputs provided to the generator:\n",
    "\n",
    "**Single-track piano roll input**: A single melody track of size (128, 128, 1) => (TimeStep, NumPitches, NumTracks) is provided as the input to the model. \n",
    "\n",
    "Notice from the figure below that the encoding layers of the model on the left side and decoder layer on on the right side are connected to create a U-shape, thereby giving the name U-Net to this architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/unet.png\" alt=\"Model architecture\" width=\"800\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build The Model\n",
    "class ArCnnModel():\n",
    "    def __init__(self,\n",
    "                 input_dim,\n",
    "                 num_filters,\n",
    "                 growth_factor,\n",
    "                 num_layers,\n",
    "                 dropout_rate_encoder,\n",
    "                 dropout_rate_decoder,\n",
    "                 batch_norm_encoder,\n",
    "                 batch_norm_decoder,\n",
    "                 learning_rate,\n",
    "                 optimizer_enum,\n",
    "                 pre_trained=None):\n",
    "\n",
    "        # Piano roll Input Dimensions\n",
    "        self.input_dim = input_dim\n",
    "        # Number of filters in the convolution\n",
    "        self.num_filters = num_filters\n",
    "        # Growth rate of number of filters at each convolution\n",
    "        self.growth_factor = growth_factor\n",
    "        # Number of Encoder and Decoder layers\n",
    "        self.num_layers = num_layers\n",
    "        # A list of dropout values at each encoder layer\n",
    "        self.dropout_rate_encoder = dropout_rate_encoder\n",
    "        # A list of dropout values at each decoder layer\n",
    "        self.dropout_rate_decoder = dropout_rate_decoder\n",
    "        # A list of flags to ensure if batch_nromalization at each encoder\n",
    "        self.batch_norm_encoder = batch_norm_encoder\n",
    "        # A list of flags to ensure if batch_nromalization at each decoder\n",
    "        self.batch_norm_decoder = batch_norm_decoder\n",
    "        # Path to pretrained Model\n",
    "        self.pre_trained = pre_trained\n",
    "        # Learning rate for the model\n",
    "        self.learning_rate = learning_rate\n",
    "        # Optimizer to use while training the model\n",
    "        self.optimizer_enum = optimizer_enum\n",
    "        if self.num_layers < 1:\n",
    "            raise ValueError(\n",
    "                \"Number of layers should be greater than or equal to 1\")\n",
    "\n",
    "    # Number of times Conv2D to be performed\n",
    "    CONV_PER_LAYER = 2\n",
    "\n",
    "    def down_sampling(self,\n",
    "                      layer_input,\n",
    "                      num_filters,\n",
    "                      batch_normalization=False,\n",
    "                      dropout_rate=0):\n",
    "        '''\n",
    "        :param: layer_input: Input Layer to the downsampling block\n",
    "        :param: num_filters: Number of filters\n",
    "        :param: batch_normalization: Flag to check if batch normalization to be performed\n",
    "        :param: dropout_rate: To regularize overfitting\n",
    "        '''\n",
    "        encoder = layer_input\n",
    "        for _ in range(self.CONV_PER_LAYER):\n",
    "            encoder = Conv2D(num_filters, (3, 3),\n",
    "                             activation='relu',\n",
    "                             padding='same')(encoder)\n",
    "            pooling_layer = MaxPooling2D(pool_size=(2, 2))(encoder)\n",
    "            if dropout_rate:\n",
    "                pooling_layer = Dropout(dropout_rate)(pooling_layer)\n",
    "            if batch_normalization:\n",
    "                pooling_layer = BatchNormalization()(pooling_layer)\n",
    "        return encoder, pooling_layer\n",
    "\n",
    "    def up_sampling(self,\n",
    "                    layer_input,\n",
    "                    skip_input,\n",
    "                    num_filters,\n",
    "                    batch_normalization=False,\n",
    "                    dropout_rate=0):\n",
    "        '''\n",
    "        :param: layer_input: Input Layer to the downsampling block\n",
    "        :param: num_filters: Number of filters\n",
    "        :param: batch_normalization: Flag to check if batch normalization to be performed\n",
    "        :param: dropout_rate: To regularize overfitting\n",
    "        '''\n",
    "        decoder = concatenate(\n",
    "            [UpSampling2D(size=(2, 2))(layer_input), skip_input])\n",
    "        if batch_normalization:\n",
    "            decoder = BatchNormalization()(decoder)\n",
    "        for _ in range(self.CONV_PER_LAYER):\n",
    "            decoder = Conv2D(num_filters, (3, 3),\n",
    "                             activation='relu',\n",
    "                             padding='same')(decoder)\n",
    "\n",
    "        if dropout_rate:\n",
    "            decoder = Dropout(dropout_rate)(decoder)\n",
    "        return decoder\n",
    "\n",
    "    def get_optimizer(self, optimizer_enum, learning_rate):\n",
    "        '''\n",
    "        Use either Adam or RMSprop.\n",
    "        '''\n",
    "        if OptimizerType.ADAM == optimizer_enum:\n",
    "            optimizer = Adam(lr=learning_rate)\n",
    "        elif OptimizerType.RMSPROP == optimizer_enum:\n",
    "            optimizer = RMSprop(lr=learning_rate)\n",
    "        else:\n",
    "            raise Exception(\"Only Adam and RMSProp optimizers are supported\")\n",
    "        return optimizer\n",
    "\n",
    "    def build_model(self):\n",
    "        # Create a list of encoder sampling layers\n",
    "        down_sampling_layers = []\n",
    "        up_sampling_layers = []\n",
    "        inputs = Input(self.input_dim)\n",
    "        layer_input = inputs\n",
    "        num_filters = self.num_filters\n",
    "        # encoder samplimg layers\n",
    "        for layer in range(self.num_layers):\n",
    "            encoder, pooling_layer = self.down_sampling(\n",
    "                layer_input=layer_input,\n",
    "                num_filters=num_filters,\n",
    "                batch_normalization=self.batch_norm_encoder[layer],\n",
    "                dropout_rate=self.dropout_rate_encoder[layer])\n",
    "\n",
    "            down_sampling_layers.append(encoder)\n",
    "            layer_input = pooling_layer  # Get the previous pooling_layer_input\n",
    "            num_filters *= self.growth_factor\n",
    "\n",
    "        # bottle_neck layer\n",
    "        bottle_neck = Conv2D(num_filters, (3, 3),\n",
    "                             activation='relu',\n",
    "                             padding='same')(pooling_layer)\n",
    "        bottle_neck = Conv2D(num_filters, (3, 3),\n",
    "                             activation='relu',\n",
    "                             padding='same')(bottle_neck)\n",
    "        num_filters //= self.growth_factor\n",
    "\n",
    "        # upsampling layers\n",
    "        decoder = bottle_neck\n",
    "        for index, layer in enumerate(reversed(down_sampling_layers)):\n",
    "            decoder = self.up_sampling(\n",
    "                layer_input=decoder,\n",
    "                skip_input=layer,\n",
    "                num_filters=num_filters,\n",
    "                batch_normalization=self.batch_norm_decoder[index],\n",
    "                dropout_rate=self.dropout_rate_decoder[index])\n",
    "            up_sampling_layers.append(decoder)\n",
    "            num_filters //= self.growth_factor\n",
    "\n",
    "        output = Conv2D(1, 1, activation='linear')(up_sampling_layers[-1])\n",
    "        model = Model(inputs=inputs, outputs=output)\n",
    "        optimizer = self.get_optimizer(self.optimizer_enum, self.learning_rate)\n",
    "        model.compile(optimizer=optimizer, loss=Loss.built_in_softmax_kl_loss)\n",
    "        if self.pre_trained:\n",
    "            model.load_weights(self.pre_trained)\n",
    "        model.summary()\n",
    "        return model\n",
    "\n",
    "\n",
    "class OptimizerType(Enum):\n",
    "    ADAM = \"Adam\"\n",
    "    RMSPROP = \"RMSprop\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "We split the dataset into training and validation sets. The default training-validation split is 0.9, but this can be changed with the parameter **“training_validation_split”** in **constants.py**.\n",
    "\n",
    "During training, the data generator creates (input, target) pairs by applying augmentations on the piano rolls present in the dataset. Details of the augmentation are described in the previous section. In each epoch, different notes will be added and removed from original piano rolls to form the augmented piano rolls (as these notes are added/removed in random spots each time). This means that we will have a new set of augmented piano rolls for each epoch, and this effectively creates an unlimited input training dataset. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_size = len(dataset_samples)\n",
    "dataset_split = math.floor(dataset_size * Constants.training_validation_split)\n",
    "print(0, dataset_split, dataset_split + 1, dataset_size)\n",
    "\n",
    "training_samples = dataset_samples[0:dataset_split]\n",
    "print(\"training samples length: {}\".format(len(training_samples)))\n",
    "validation_samples = dataset_samples[dataset_split + 1:dataset_size]\n",
    "print(\"validation samples length: {}\".format(len(validation_samples)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the ArCnn model related hyperparameters can be changed from below. For instance, to decrease the model size, change the default value of num_layers from 5, and update the dropout_rate_encoder, dropout_rate_deoder, batch_norm_encoder and batch_norm_decoder lists accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Piano Roll Input Dimensions\n",
    "input_dim = (Constants.bars * Constants.beats_per_bar * Constants.beat_resolution, \n",
    "             Constants.number_of_pitches, \n",
    "             Constants.number_of_channels)\n",
    "# Number of Filters In The Convolution\n",
    "num_filters = 32\n",
    "# Growth Rate Of Number Of Filters At Each Convolution\n",
    "growth_factor = 2\n",
    "# Number Of Encoder And Decoder Layers\n",
    "num_layers = 5\n",
    "# A List Of Dropout Values At Each Encoder Layer\n",
    "dropout_rate_encoder = [0, 0.5, 0.5, 0.5, 0.5]\n",
    "# A List Of Dropout Values At Each Decoder Layer\n",
    "dropout_rate_decoder = [0.5, 0.5, 0.5, 0.5, 0]\n",
    "# A List Of Flags To Ensure If batch_normalization Should be performed At Each Encoder\n",
    "batch_norm_encoder = [True, True, True, True, False]\n",
    "# A List Of Flags To Ensure If batch_normalization Should be performed At Each Decoder\n",
    "batch_norm_decoder = [True, True, True, True, False]\n",
    "# Path to Pretrained Model If You Want To Initialize Weights Of The Network With The Pretrained Model\n",
    "pre_trained = False\n",
    "# Learning Rate Of The Model\n",
    "learning_rate = 0.001\n",
    "# Optimizer To Use While Training The Model\n",
    "optimizer_enum = OptimizerType.ADAM\n",
    "# Batch Size\n",
    "batch_size = 32\n",
    "# Number Of Epochs\n",
    "epochs = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Number of Batch Iterations Before A Training Epoch Is Considered Finsihed\n",
    "steps_per_epoch = int(\n",
    "    len(training_samples) * Constants.samples_per_ground_truth_data_item / int(batch_size))\n",
    "\n",
    "print(\"The Total Number Of Steps Per Epoch Are: \"+ str(steps_per_epoch))\n",
    "\n",
    "# Total Number Of Time Steps\n",
    "n_timesteps = Constants.bars * Constants.beat_resolution * Constants.beats_per_bar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build The Data Generators\n",
    "\n",
    "Now let's build the training and validation data generators to create data on the fly during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Training Data Generator\n",
    "training_data_generator = PianoRollGenerator(sample_list=training_samples,\n",
    "                                             sampling_lower_bound_remove = sampling_lower_bound_remove,\n",
    "                                             sampling_upper_bound_remove = sampling_upper_bound_remove,\n",
    "                                             sampling_lower_bound_add = sampling_lower_bound_add,\n",
    "                                             sampling_upper_bound_add = sampling_upper_bound_add,\n",
    "                                             batch_size = batch_size,\n",
    "                                             bars = Constants.bars,\n",
    "                                             samples_per_data_item = Constants.samples_per_ground_truth_data_item,\n",
    "                                             beat_resolution = Constants.beat_resolution,\n",
    "                                             beats_per_bar = Constants.beats_per_bar,\n",
    "                                             number_of_pitches = Constants.number_of_pitches,\n",
    "                                             number_of_channels = Constants.number_of_channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vaalidation Data Generator\n",
    "validation_data_generator = PianoRollGenerator(sample_list = validation_samples,\n",
    "                                               sampling_lower_bound_remove = sampling_lower_bound_remove,\n",
    "                                               sampling_upper_bound_remove = sampling_upper_bound_remove,\n",
    "                                               sampling_lower_bound_add = sampling_lower_bound_add,\n",
    "                                               sampling_upper_bound_add = sampling_upper_bound_add,\n",
    "                                               batch_size = batch_size, \n",
    "                                               bars = Constants.bars,\n",
    "                                               samples_per_data_item = Constants.samples_per_ground_truth_data_item,\n",
    "                                               beat_resolution = Constants.beat_resolution,\n",
    "                                               beats_per_bar = Constants.beats_per_bar, \n",
    "                                               number_of_pitches = Constants.number_of_pitches,\n",
    "                                               number_of_channels = Constants.number_of_channels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Callbacks for the model. \n",
    "1. Create **Training Vs Validation** loss plots during training.\n",
    "2. Save model checkpoints based on the **Best Validation Loss**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callback For Loss Plots \n",
    "plot_losses = GenerateTrainingPlots()\n",
    "## Checkpoint Path\n",
    "checkpoint_filepath =  'checkpoints/-best-model-epoch:{epoch:04d}.hdf5'\n",
    "\n",
    "# Callback For Saving Model Checkpoints \n",
    "model_checkpoint_callback = keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    save_weights_only=False,\n",
    "    monitor='val_loss',\n",
    "    mode='min',\n",
    "    save_best_only=True)\n",
    "\n",
    "# Create A List Of Callbacks\n",
    "callbacks_list = [plot_losses, model_checkpoint_callback]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create A Model Instance\n",
    "MusicModel = ArCnnModel(input_dim = input_dim,\n",
    "                        num_filters = num_filters,\n",
    "                        growth_factor = growth_factor,\n",
    "                        num_layers = num_layers,\n",
    "                        dropout_rate_encoder = dropout_rate_encoder,\n",
    "                        dropout_rate_decoder = dropout_rate_decoder,\n",
    "                        batch_norm_encoder = batch_norm_encoder,\n",
    "                        batch_norm_decoder = batch_norm_decoder,\n",
    "                        pre_trained = pre_trained,\n",
    "                        learning_rate = learning_rate,\n",
    "                        optimizer_enum = optimizer_enum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MusicModel.build_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start Training\n",
    "history = model.fit_generator(training_data_generator,\n",
    "                              validation_data = validation_data_generator,\n",
    "                              steps_per_epoch = steps_per_epoch,\n",
    "                              epochs = epochs,\n",
    "                              callbacks = callbacks_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Bach Like Enhanced Melody For Custom Input\n",
    "\n",
    "Congratulations! You have trained your very own AutoRegressive model to generate music. Let us see how our music model performs on a custom input.\n",
    "\n",
    "Before loading the model, we need to load inference related parameters. After that, we load our pretrained model and generate a new melody based on **\"Twinkle Twinkle Little Star\"**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inference is done by sampling from the model’s predicted probability distribution across the entire piano roll. It is an iterative process, and a note is added or removed to the input in every iteration via sampling. After adding or removing a note to the input in an iteration, this new input is fed back into the model. The model is trained to both remove and add notes, and it can improve the input melody, and can correct mistakes that it may have made in earlier iterations as well.\n",
    "\n",
    "You can change certain inference parameters to observe the differences in the generated music as described below.  \n",
    "* **Sampling iterations** - his specifies the number of iterations during inference. Larger number of sampling iterations can ensure that the model has had enough time to improve the input melody and also correct any mistakes it may have made along the way. Beyond a certain number of sampling iterations, it can be observed that the model keeps adding notes and then removing those notes in a subsequent iterations or vice-versa. This implies that convergence has been reached.\n",
    "* **Maximum Notes to Remove** - This specifies the maximum percentage of notes of the original input melody that can be removed during inference. If you choose this to be 0%, then none of your original melody will be removed during inference. \n",
    "* **Maximum Notes to Add** - This specifies the maximum number of new notes to add to the original input melody during inference.With the “Maximum Notes to Remove” and “Maximum Notes to Add”, you can choose the degree to which you would like to preserve your original input melody. However, by restricting the model’s ability to add or remove notes, you may risk losing some music quality. \n",
    "* **Creativity**- The output probability distribution generated by the model is obtained via softmax, and you can change the temperature for softmax to get different levels of “creativity”. By using lower temperatures, the output probability distribution would have more distinct peaks, and the model would be more confident in its predictions. By using higher temperatures, the output probability distribution would be flatter, and the model would have a higher chance of choosing less likely notes to add/remove. By increasing the temperature, you can give the model the ability to take more risks, and increase its “creativity”.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us first load our last saved or pretrained checkpoint and inference related parameters. To modify the inference related parameters, please navigate to **inference_parameters.json** and change the values in the json file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load The Inference Related Parameters\n",
    "with open('inference_parameters.json') as json_file:\n",
    "    inference_params = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create An Inference Object\n",
    "inference_obj = Inference()\n",
    "# Load The Checkpoint\n",
    "inference_obj.load_model('checkpoints/-best-model-epoch:0001.hdf5')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please navigate to **sample_inputs** directory to find different input melodies we have already created for you to help generating novel compositions.\n",
    "\n",
    "To download the novel compositions, you have created using the model we just trained, please navigate to **outputs** directory and download the midi file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate The Composition\n",
    "inference_obj.generate_composition('sample_inputs/twinkle_twinkle.midi', inference_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now, Let's Play The Generated Output And Listen To It"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "play_midi(\"outputs/output_0.mid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Results\n",
    "\n",
    "Now that we have finished generating our enhanced melody, let's find out how we did. We will analyze our output using below three metrics and compare them with the sample input:\n",
    "\n",
    "- **Empty Bar Rate:** The ratio of empty bars to total number of bars.\n",
    "- **Pitch Histogram Distance:** A metric that captures the distribution and position of pitches.\n",
    "- **In Scale Ratio:** Ratio of the number of notes that are in C major key, which is a common key found in music, to the total number of notes. \n",
    "\n",
    "After computing the metrics, let's also visualize the input piano roll and compare it with the generated output piano roll to notice the notes added.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input Midi Metrics:\n",
    "print(\"The input midi metrics are:\")\n",
    "get_music_metrics(\"sample_inputs/twinkle_twinkle.midi\", beat_resolution=4)\n",
    "\n",
    "print(\"\\n\")\n",
    "# Generated Output Midi Metrics:\n",
    "print(\"The generated output midi metrics are:\")\n",
    "get_music_metrics(\"outputs/output_0.mid\", beat_resolution=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert The Input and Generated Midi To Tensors\n",
    "input_pianoroll = process_midi(\"sample_inputs/twinkle_twinkle.midi\", beat_resolution=4)\n",
    "output_pianoroll = process_midi(\"outputs/output_0.mid\", beat_resolution=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Input Piano Roll\n",
    "plot_pianoroll(input_pianoroll, beat_resolution=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Output Piano Roll\n",
    "plot_pianoroll(output_pianoroll, beat_resolution=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Open Source Implementations\n",
    "For more open-source implementations of generative models for music, check out:\n",
    "\n",
    "- [MuseNet](https://openai.com/blog/musenet/): Uses GPT2, a large-scale Transformer model to predict the next token in sequence\n",
    "- [Jukebox](https://openai.com/blog/jukebox/): Uses various neural nets to generate music, including rudimentary singing, as raw audio in a variety of genres and artist styles.\n",
    "- [Music Transformer](https://github.com/tensorflow/magenta/tree/master/magenta/models/score2perf): Uses transformers to generate music!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='references'></a>\n",
    "1. [MuseGAN: Multi-track Sequential Generative Adversarial Networks for Symbolic Music Generation and Accompaniment.](https://arxiv.org/abs/1709.06298)\n",
    "2. [MidiNet: A Convolutional Generative Adversarial Network for Symbolic-domain Music Generation.](https://arxiv.org/abs/1703.10847)\n",
    "3. [A Hierarchical Recurrent Neural Network for Symbolic Melody Generation.](https://pubmed.ncbi.nlm.nih.gov/31796422/)\n",
    "4. [Counterpoint by Convolution](https://arxiv.org/abs/1903.07227)\n",
    "5. [MusicTransformer:Generating Music With Long-Term Structure](https://arxiv.org/abs/1809.04281)\n",
    "6. [Conditional Image Generation with PixelCNN Decoders](https://arxiv.org/abs/1606.05328)\n",
    "7. [Neural Autoregressive Distribution Estimation](https://arxiv.org/abs/1605.02226)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow_p36",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
